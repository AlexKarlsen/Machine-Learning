Life below water is a critical ecosystem, that experiences a lot of challenging changes, which is way it has found its way to the UN Sustainable Goals \cite{UN}. The Finnish Environment Institute \cite{meissner} SYKE claims, that the loss of aquatic biodiversity and associated Ecosystem Services is one of the most pressing problems on Earth. The DETECT project aims to identify the various species of aquatic macroinvertebrates by imagery and classification using computer vision and machine learning. 

This project is a part of the Computer Vision and Machine Learning course at Aarhus University. The project is run as a Kaggle Challenge with the aim of improving the classification accuracy on the data set. The data set is available for download on the challenge page on Kaggle. The data set consists of raw images of 29 different classes of aquaitc macroinvertebrates and is split in training-, validation- and test set. Training- and validation set consist of correspendingly 5830 and 2298 labeled images and the test set consists of 3460 unlabeled images. The results of the test set are uploaded to Kaggle, where the results are scored by comparing with withhold ground truth labels. 

All the data set are available as feature vectors, that have been extracted from a pretrained AlexNet. AlexNet \cite{Krizhevsky:2012} was the first convolutional neural network (CNN) used on the ImageNet challenge, and since then CNNs have become the golden standard to obtain remarkable classification results. The AlexNet features for this project are used to train both linear and non-linear classifiers to evaluate different classification methods on this data set and get hands-on experience with a real classification challenge. 

Additionally the project examines a more modern CNN, InceptionV3 \cite{inception} pretrained on the ImageNet data set, is used as a feature extractor. The same classification methods are used to evaluate importance of features extraction og images. 

Among the different classification techniques used are; K-Nearest Neighbor (kNN), Linear Discriminat Analysis (LDA), Support Vector Machines (SVM) and Neural Networks (NN). Principal Component Analysis (PCA) are used for dimensionalty reduction and Search Grid are used for hyper-parameter optimization.

The rest of the paper is structured as follows; section \ref{sec:dataset} desribes the data set, section \ref{sec:methods} describes the methods and frameworks used in the project, section \ref{sec:results} describes the result of the project, section \ref{sec:discussion} is a discussion of the results and last in section \ref{sec:conclusion} the project is concluded upon.